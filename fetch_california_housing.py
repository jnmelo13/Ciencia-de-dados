# -*- coding: utf-8 -*-
"""fetch_california_housing

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t6zLOsJzvD7IQp8sXr_2ks8NDPOKL7qc

# Importando Bibliotecas
"""

#Fazendo a importação dos pacotes
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import statsmodels.api as sm
from statsmodels.formula.api import ols
from sklearn.preprocessing import StandardScaler
import xgboost as xgb
import sklearn
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.naive_bayes import GaussianNB
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor
from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, roc_curve
import warnings
from sklearn import metrics
warnings.filterwarnings("ignore")

!pip install scikit-learn
from sklearn.tree import DecisionTreeClassifier

"""#Carregando Dados"""

df = pd.read_csv('/content/housing.csv')

#verificando tamanho do dataset
df.shape

#Verificando tipo dos dados
df.dtypes

#Verificando uma amostra dos dados
df.head()

"""#Análise Exploratória"""

# Criando um dicionário com a descrição de cada variável
df_dict = {"longitude":"Coordenada de longitude do endereço",
           "latitude": "Coordenada de latitude do endereço",
            "housingMedianAge": "Idade média das construções",
            "totalRooms": "Quantidade de comôdos em um quarteirão",
            "totalBedrooms": "Quantidade de quartos em um quarteirão",
            "population": "Número total de pessoas que residem em um quarteirão",
            "households": "Quantidade de famílias que residem em um quarteirão",
            "medianIncome": "Renda média das famílias dentro de um quarteirão (medida em dezenas de milhares de dólares americanos)",
            "medianHouseValue": "Preço médio das residências no quarteirão",
            "oceanProximity": "O quão próximo a o quarteirão é do mar"}

df_dict['population']

#Histograma da variável alvo
df['median_house_value'].hist();

#Resumo estatístico dos dados
df.describe()

#Dados da única variável categórica
df['ocean_proximity'].value_counts().plot(kind = 'bar');

# Função para o plot da relação da variável alvo com alguns atributos
def get_pairs(dataframe, alvo, atributos, n):

    # Grupos de linhas com 3 (n) gráficos por linha
    grupos_linhas = [atributos[i:i+n] for i in range(0, len(atributos), n)]

    # Loop pelos grupos de linhas para criar cada pair plot
    for linha in grupos_linhas:
        plot = sns.pairplot(x_vars = linha, y_vars = alvo, data = dataframe, kind = "reg", height = 3)

    return

alvo = ['median_house_value']
atributos = ['longitude','latitude','housing_median_age','total_rooms','total_bedrooms','population','households','median_income']

get_pairs(df,alvo,atributos,3)

#Visualizando a relação da variável categórica com a variável alvo
df.groupby('ocean_proximity')['median_house_value'].mean().plot(kind = 'bar');

pd.get_dummies(df['ocean_proximity'])

df = pd.concat([df.drop('ocean_proximity',axis = 1),pd.get_dummies(df['ocean_proximity'])],axis = 1)
df

"""#Estudando Valores Ausentes"""

df.count()

# Extraindo o total de valores ausentes por coluna
valores_ausentes = df.isnull().sum().sort_values(ascending = False)
valores_ausentes

# Calculando o percentual de valores ausentes
valores_ausentes_percent = valores_ausentes[valores_ausentes > 0] / df.shape[0]
valores_ausentes_percent

#Percentual de valores ausentes ~1%, portanto descartaremos da análise
df_novo = df.dropna()

df_novo.count()

"""#Estudando Outliers"""

# Substituindo True e False por 1 e 0 respectivamente
df_novo = df_novo.replace({True: 1, False: 0})

# Identificação de outliers através do método do IQR (Interquartile Range)
Q1 = df_novo.quantile(0.25)
Q3 = df_novo.quantile(0.75)
IQR = Q3 - Q1

# Definir outliers como aqueles valores que estão fora do intervalo [Q1 - 1.5 * IQR, Q3 + 1.5 * IQR]
outliers = ((df_novo < (Q1 - 1.5 * IQR)) | (df_novo > (Q3 + 1.5 * IQR))).sum()
outliers

# Cria um resumo com os outliers
outliers_summary = pd.DataFrame({'Outliers': outliers, 'Percentual': (outliers / len(df_novo)) * 100})
# Retorna os resultados quando o valor for maior do que zero
outliers_summary[outliers_summary['Outliers'] > 0]

"""#Análise Estatística dos Outliers"""

#Criando dataframe sem os outliers
df_sem_outliers = df_novo[~((df_novo < (Q1 - 1.5 * IQR)) | (df_novo > (Q1 + 1.5 * IQR))).any(axis=1)]
df_sem_outliers

get_pairs(df_sem_outliers,alvo,atributos,3)

"""Consideração Importante:
- Ao retirar os Outliers com o filtro de 1.5 IQR, a distribuição no gráfico de dispersão passou a não demostrar correlação entre as variáveis preditoras e a variável alvo. Portanto, não faz sentido seguir com o dataset expurgando os outliers

#Engenharia de Recursos
"""

#Criando novos atributos
df_novo['income_range'] = 1
df_novo['pop_size'] = 1
df_novo['bedrooms_per_household'] = df_novo['total_bedrooms']/df_novo['households']
df_novo['bedrooms_ratio'] = df_novo['total_bedrooms']/df_novo['total_rooms']

# Criando os pontos de corte (cutoffs) com 2 percentis como referência
pontos_corte_population = [df_novo.population.quantile(0.33), df_novo.population.quantile(0.67)]
pontos_corte_median_income = [df_novo.median_income.quantile(0.33), df_novo.median_income.quantile(0.67)]

pontos_corte_population

# Definindo a faixa onde a variável terá o valor 2 e 3
df_novo.loc[(df_novo.population < pontos_corte_population[1]) & (df_novo.population > pontos_corte_population[0]), 'pop_size'] = 2
df_novo.loc[df_novo.population > pontos_corte_population[1], 'pop_size'] = 3
df_novo

pontos_corte_median_income

# Definindo a faixa onde a variável terá o valor 2 e 3
df_novo.loc[(df_novo.median_income < pontos_corte_median_income[1]) & (df_novo.median_income > pontos_corte_median_income[0]), 'income_range'] = 2
df_novo.loc[df_novo.median_income > pontos_corte_median_income[1], 'income_range'] = 3
df_novo

df_novo[df_novo['bedrooms_per_household'] == df_novo['bedrooms_per_household'].max()]

"""#Análise de Multicolinearidade

Para desenvolver um bom modelo, desejamos que as variáveis preditoras tenham uma alta correlação com a variável alvo, mas não uma alta correlação entre si, pois isso fere uma das premissas da regressão.

OBS: Correlação não indica causalidade
"""

plt.figure(figsize = (16,10))
sns.heatmap(df_novo.corr(),annot = True,cmap = 'seismic');

#Esudando correlação com a variável alvo
corr_matrix = df_novo.corr(numeric_only=True)
corr_matrix[(corr_matrix["median_house_value"]>0.15) | (corr_matrix["median_house_value"]<-0.15)]["median_house_value"]

# Filtrando apenas as correlações significativas
dfCorr = df_novo.corr()
df_filtro_1 = dfCorr[((dfCorr >= 0.15) | (dfCorr <= -0.15)) & (dfCorr != 1.000)]
df_filtro_1

"""As variáveis median_income, income_range,bedrooms_ratio,<1H OCEAN e INLAND, NEAR BAY são as que apresentam correlação mais significativa com a variável alvo

Para garantir que não teremos problemas de multicolinearidade, vamos agora checar as correlações >=  65% e escolher uma das duas variáveis nos casos que forem encontrados
"""

# Filtrando a matriz de correlação
dfCorr = df_novo.corr()
df_filtro_2 = dfCorr[((dfCorr >= 0.65) | (dfCorr <= -0.65)) & (dfCorr != 1.000)]
df_filtro_2 = df_filtro_2.drop('median_house_value', axis=1)
df_filtro_2 = df_filtro_2.drop('median_house_value', axis=0)
df_filtro_2

"""##Explicando Decisão

**Primeiro Filtro - Correlação com a variável alvo**

As variáveis  median_income, income_range,bedrooms_ratio, <1H OCEAN, INLAND e NEAR BAY são as que atendem ao critério utilizado

**Segundo Filtro - Multicolinearidade**

Entre as variáveis que apresentaram alta correlação entre si (critério de 0.65), foram escolhidas as que apresentaram maior correlação com a variável alvo. São elas: median_income,<1H OCEAN, INLAND e NEAR BAY
"""

# Preparando o novo dataset
df_novo_final = pd.DataFrame({'median_house_value': df_novo['median_house_value'],
                              'median_income': df_novo['median_income'],
                              'next_ocean': df_novo['<1H OCEAN'],
                              'inland': df_novo['INLAND'],
                              'near_bay': df_novo['NEAR BAY'],
                              'bedrooms_ratio':df_novo['bedrooms_ratio'],})

df_novo_final.sample(10)

#Checando valores ausentes
df_novo_final.count()

df_novo_final.dtypes

"""#Modelagem Estatística"""

# Variável alvo x Variáveis Preditoras
formula = 'median_house_value ~ median_income + next_ocean + inland + near_bay + bedrooms_ratio'

# Cria e treina o modelo
modelo_v1 = ols(formula, data = df_novo_final).fit()

# Coeficiente de Determinação
print(f"R-squared: {modelo_v1.rsquared}")

# Coeficientes
print(f"Coeficientes:\n{modelo_v1.params}")

modelo_v1.summary()

# Padronização
scaler = StandardScaler()
scaler.fit(df_novo_final[['median_income']])

df_novo_final['median_income_padronizada'] = scaler.transform(df_novo_final[['median_income']])

# Variável alvo x Variáveis Preditoras
formula = 'median_house_value ~ median_income_padronizada + next_ocean + inland + bedrooms_ratio'
# Cria e treina o modelo
modelo_v2 = ols(formula, data = df_novo_final).fit()
modelo_v2.summary()

"""##Conclusão da modelagem estatística

As variáveis median_income,median_income,next_ocean,inland, near_bay e bedrooms_ratio 60% das variação da variável alvo median_house_value

#Trabalhando com o df sem outliers - Não faz sentido

##Engenharia de recursos
"""

df_sem_outliers['bedrooms_ratio'] = df_sem_outliers['total_bedrooms']/df_novo['total_rooms']

df_sem_outliers

"""##Análise de Multicolinearidade"""

df_sem_outliers[df_sem_outliers['ISLAND']==0]

plt.figure(figsize = (16,10))
sns.heatmap(df_sem_outliers.corr(),annot = True,cmap = 'seismic');

#Esudando correlação com a variável alvo
corr_matrix = df_sem_outliers.corr(numeric_only=True)
corr_matrix[(corr_matrix["median_house_value"]>0.15) | (corr_matrix["median_house_value"]<-0.15)]["median_house_value"]

"""- As variáveis latitude, total_rooms, median_income, median_house_value, 1H OCEAN e INLAND são as que possuem maior correlação com a variável dependente
- A fim de evitar multicolinearidade, será feito um corte no valor de 0.65 na correlação entre variáveis independentes e escolhida aquela que apresentar maior correlação com a variável dependente
"""

# Filtrando a matriz de correlação
dfCorr = df_sem_outliers.corr()
df_filtro_2 = dfCorr[((dfCorr >= 0.65) | (dfCorr <= -0.65)) & (dfCorr != 1.000)]
df_filtro_2 = df_filtro_2.drop('median_house_value', axis=1)
df_filtro_2 = df_filtro_2.drop('median_house_value', axis=0)
df_filtro_2

"""- Foi retirada a variável INLAND, pois apresenta alta correlação com <1H OCEAN."""

# Preparando o novo dataset
df_sem_outliers_final = pd.DataFrame({'median_house_value': df_sem_outliers['median_house_value'],
                              'median_income': df_sem_outliers['median_income'],
                              'next_ocean': df_sem_outliers['<1H OCEAN'],
                              'latitude': df_sem_outliers['latitude'],
                              'total_rooms':df_sem_outliers['total_rooms'],})

df_sem_outliers_final.count()

# Variável alvo x Variáveis Preditoras
formula = 'median_house_value ~ median_income + next_ocean + latitude + total_rooms'

# Cria e treina o modelo
modelo_v1 = ols(formula, data = df_sem_outliers_final).fit()
modelo_v1.summary()

"""#Machine Learning

##Pré-Processamento dos Dados
"""

#Variável alvo
y = df_novo['median_house_value']
#Variáveis preditoras
X = df_novo.drop(columns = 'median_house_value')

#Dividindo entre base de treino e teste
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42)

print('Treinamento: ',X_train.shape,y_train.shape)
print('Teste: ',X_test.shape,y_test.shape)

scaler = StandardScaler()
scaler = scaler.fit(X_test)

#Padronizando dados devido a amplitude dos valores na amostra
X_test_scaled = scaler.transform(X_test)
X_train_scaled = scaler.transform(X_train)

print('Treinamento: ',X_test_scaled.shape)
print('Teste: ',X_train_scaled.shape)

"""##Modelagem Preditiva

### V1 - Modelo Benchmark
"""

#Chamando Modelo
model_v1 = LinearRegression()

#Treinando o Modelo
model_v1.fit(X_train_scaled,y_train)

# O que o modelo aprendeu:
print("Coeficientes: \n", model_v1.coef_)

# Previsões com dados de treino
y_pred_train_v1 = model_v1.predict(X_train_scaled)

# Print das métricas em treino
print('Mean Absolute Error:', metrics.mean_absolute_error(y_train, y_pred_train_v1))
print('Mean Squared Error (R²):', metrics.mean_squared_error(y_train, y_pred_train_v1))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_train, y_pred_train_v1)))
print('R2 Score:', metrics.r2_score(y_train, y_pred_train_v1))

#Avaliação do modelo
y_pred_test_v1 = model_v1.predict(X_test_scaled)

y_pred_test_v1

# Dataframe com as previsões e valores reais
df_previsoes = pd.DataFrame({'Valor_Real': y_test, 'Valor_Previsto': y_pred_test_v1})

df_previsoes.head()

# Função para criar gráfico de dispersão
def grafico_dispersao(x, y, title, xlabel, ylabel):

    # Figura e subplots
    fig, ax = plt.subplots(figsize = (10, 6))

    # Scatter
    ax.scatter(x, y, color = "blue", alpha = 0.3)

    # Labels
    ax.set_title(title)
    ax.set_xlabel(xlabel)
    ax.set_ylabel(ylabel)

    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)

    return

grafico_dispersao(y_test, y_pred_test_v1, 'Modelo V1', 'Reais', 'Previstos')

# Print das métricas com dados de teste
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred_test_v1))
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred_test_v1))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_test_v1)))
print('R2 Score:', metrics.r2_score(y_test, y_pred_test_v1))

"""###V2 - Ridge (L2)"""

model_v2 = Ridge(alpha = 1.0)

model_v2 = model_v2.fit(X_train_scaled, y_train)

y_pred_train_v2 = model_v2.predict(X_train_scaled)

print('Mean Absolute Error:', metrics.mean_absolute_error(y_train, y_pred_train_v2))
print('Mean Squared Error:', metrics.mean_squared_error(y_train, y_pred_train_v2))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_train, y_pred_train_v2)))
print('R2 Score:', metrics.r2_score(y_train, y_pred_train_v2))

#Avaliação do modelo
y_pred_test_v2 = model_v2.predict(X_test_scaled)

print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred_test_v2))
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred_test_v2))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_test_v2)))
print('R2 Score:', metrics.r2_score(y_test, y_pred_test_v2))

"""###V3 - Random Forest"""

model_v3 = RandomForestRegressor()

model_v3 = model_v3.fit(X_train_scaled, y_train)

model_v3

y_pred_train_v3 = model_v3.predict(X_train_scaled)

print('Mean Absolute Error:', metrics.mean_absolute_error(y_train, y_pred_train_v3))
print('Mean Squared Error:', metrics.mean_squared_error(y_train, y_pred_train_v3))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_train, y_pred_train_v3)))
print('R2 Score:', metrics.r2_score(y_train, y_pred_train_v3))

#Avaliação do modelo
y_pred_test_v3 = model_v3.predict(X_test_scaled)

print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred_test_v3))
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred_test_v3))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_test_v3)))
print('R2 Score:', metrics.r2_score(y_test, y_pred_test_v3))

"""###V4 XGBoost"""

model_v4= LogisticRegression()

model_v4 = model_v4.fit(X_train_scaled, y_train)

y_pred_train_v4 = model_v4.predict(X_train_scaled)

print('Mean Absolute Error:', metrics.mean_absolute_error(y_train, y_pred_train_v4))
print('Mean Squared Error:', metrics.mean_squared_error(y_train, y_pred_train_v4))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_train, y_pred_train_v4)))
print('R2 Score:', metrics.r2_score(y_train, y_pred_train_v4))

y_pred_test_v4 = model_v4.predict(X_test_scaled)

"""###V5 - Random Forest (Otimização de Hiperparametros)"""

#Importando GridSearchCV
from sklearn.model_selection import GridSearchCV

# Definição do espaço de hiperparâmetros para otimização
parametros = {'n_estimators': [30, 50, 70, 90, 95, 100],
              'max_depth': [5,7,10,15,17,19]}

# Configuração do GridSearchCV
grid_search = GridSearchCV(estimator = model_v3,
                               param_grid = parametros,
                               cv = 5,
                               scoring = 'neg_mean_squared_error',
                               verbose = 1)

grid_search.fit(X_train_scaled, y_train)

grid_results = pd.DataFrame(grid_search.cv_results_)
grid_results.head()

#Melhor Modelo definido pelo Grid
print(f'Melhores parametros {grid_search.best_params_}')
model_v5 = grid_search.best_estimator_

y_pred_train_v5 = model_v5.predict(X_train_scaled)

# Métricas em treino
print('Mean Absolute Error:', metrics.mean_absolute_error(y_train, y_pred_train_v5))
print('Mean Squared Error:', metrics.mean_squared_error(y_train, y_pred_train_v5))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_train, y_pred_train_v5)))
print('R2 Score:', metrics.r2_score(y_train, y_pred_train_v5))

y_pred_test_v5 = model_v5.predict(X_test_scaled)

print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred_test_v5))
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred_test_v5))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_test_v5)))
print('R2 Score:', metrics.r2_score(y_test, y_pred_test_v5))

#Testando com novos parametros
parametros = {'n_estimators': [95, 110,120,130],
              'max_depth': [17,21,25,30]}
# Configuração do GridSearchCV
grid_search_v2 = GridSearchCV(estimator = model_v3,
                               param_grid = parametros,
                               cv = 5,
                               scoring = 'neg_mean_squared_error',
                               verbose = 1)

grid_search_v2.fit(X_train_scaled, y_train)

#Melhor Modelo definido pelo Grid
print(f'Melhores parametros {grid_search_v2.best_params_}')
model_v5_2 = grid_search_v2.best_estimator_

y_pred_train_v5_2 = model_v5_2.predict(X_train_scaled)

# Métricas em treino
print('Mean Absolute Error:', metrics.mean_absolute_error(y_train, y_pred_train_v5_2))
print('Mean Squared Error:', metrics.mean_squared_error(y_train, y_pred_train_v5_2))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_train, y_pred_train_v5_2)))
print('R2 Score:', metrics.r2_score(y_train, y_pred_train_v5_2))

y_pred_test_v5_2 = model_v5_2.predict(X_test_scaled)

print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred_test_v5_2))
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred_test_v5_2))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_test_v5_2)))
print('R2 Score:', metrics.r2_score(y_test, y_pred_test_v5_2))

"""###V6 - Árvore de decisão"""

model_v6 = DecisionTreeClassifier()

model_v6 = model_v6.fit(X_train_scaled, y_train)

y_pred_train_v6 = model_v6.predict(X_train_scaled)

print('Mean Absolute Error:', metrics.mean_absolute_error(y_train, y_pred_train_v6))
print('Mean Squared Error:', metrics.mean_squared_error(y_train, y_pred_train_v6))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_train, y_pred_train_v6)))
print('R2 Score:', metrics.r2_score(y_train, y_pred_train_v6))

y_pred_test_v6 = model_v6.predict(X_test_scaled)

print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred_test_v6))
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred_test_v6))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_test_v6)))
print('R2 Score:', metrics.r2_score(y_test, y_pred_test_v6))

"""###Seleção do Modelo"""

#Compilando dados em um dataframe
# Preparando o novo dataset
resumo_resultado = pd.DataFrame({'modelo': ['LinearRegression','Ridge (L2)', 'RandomForest', 'XGBoost', 'RandomForestHyperParam_1', 'RandomForestHyperParam_2'],
                              'RMSE': [np.sqrt(metrics.mean_squared_error(y_test, y_pred_test_v1)),np.sqrt(metrics.mean_squared_error(y_test, y_pred_test_v2)),np.sqrt(metrics.mean_squared_error(y_test, y_pred_test_v3)),np.sqrt(metrics.mean_squared_error(y_test, y_pred_test_v4)),np.sqrt(metrics.mean_squared_error(y_test, y_pred_test_v5)),np.sqrt(metrics.mean_squared_error(y_test, y_pred_test_v5_2))],
                              'R2 Score': [ metrics.r2_score(y_test, y_pred_test_v1),metrics.r2_score(y_test, y_pred_test_v2),metrics.r2_score(y_test, y_pred_test_v3),metrics.r2_score(y_test, y_pred_test_v4),metrics.r2_score(y_test, y_pred_test_v5),metrics.r2_score(y_test, y_pred_test_v5_2)],
                              })
resumo_resultado.sort_values(by = 'RMSE')

#Plotando dispersao para RandomForest
grafico_dispersao(y_test, y_pred_test_v5, 'Modelo V5', 'Reais', 'Previstos')

"""###Decisão

Apesar do melhor resultado ser o apresentado pelo modelo RandomForest com ajuste de hiperparâmetros, no entanto a diferença é extramamente baixa quando comparado com o RandomForest simples. Portanto, a decisão é seguir com o modelo mais simples do RandomForest, o qual vai demandar menos recurso computacional e é mais simples de se explicar.
"""